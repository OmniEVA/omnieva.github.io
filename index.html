<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>OmniEVA: Embodied Versatile PlAnner via Task-Adaptive 3D-Grounded and Embodiment-aware Omni-Reasoning</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <link rel="stylesheet" href="assets/style.css">
  <style>
    /* Modern CSS Reset */
    *, *::before, *::after {
      box-sizing: border-box;
    }

    body {
      font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      margin: 0;
      padding: 0;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      color: #333;
      line-height: 1.6;
      overflow-x: hidden;
    }

    /* Navigation */
    .nav {
      position: fixed;
      top: 0;
      left: 0;
      right: 0;
      background: rgba(255, 255, 255, 0.95);
      backdrop-filter: blur(10px);
      z-index: 1000;
      padding: 0.8rem 0;
      border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    }

    .nav-container {
      max-width: 1152px;
      margin: 0 auto;
      padding: 0 2rem;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }

    .nav-brand {
      font-weight: 700;
      font-size: 1.2rem;
      color: #667eea;
      text-decoration: none;
    }

    .nav-links {
      display: flex;
      gap: 2rem;
      list-style: none;
      margin: 0;
      padding: 0;
    }

    .nav-links a {
      text-decoration: none;
      color: #555;
      font-weight: 500;
      transition: color 0.3s ease;
      padding: 0.5rem 1rem;
      border-radius: 15px;
      transition: all 0.3s ease;
    }

    .nav-links a:hover {
      color: #667eea;
      background: rgba(102, 126, 234, 0.1);
    }

    /* Hero Section */
    .hero {
      background: white;
      color: black;
      padding: 1.5rem 2rem 1rem; /* Reduced padding to show content earlier */
      text-align: center;
      position: relative;
      overflow: hidden;
    }

    .hero-content {
      position: relative;
      z-index: 1;
      max-width: 1400px;
      margin: 0 auto;
      padding: 0 2rem;
    }

    .hero h1 {
      font-size: clamp(1.6rem, 3.2vw, 2.6rem);
      font-weight: 700;
      margin: 0 0 0.5rem;
      color: black;
      line-height: 1.1;
      min-width: 600px;
      flex: 1;
    }

    .highlight-letter {
      position: relative;
      display: inline-block;
      color: #667eea;
      font-weight: 900;
    }

    @keyframes fadeInUp {
      from {
        opacity: 0;
        transform: translateY(30px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    .hero .subtitle {
      font-size: 1.3rem;
      font-weight: 300;
      margin: 0 0 1rem;
      color: #333;
    }

    /* Title container styling */
    .title-container {
      display: flex;
      align-items: center;
      justify-content: center;
      gap: 1.5rem;
      margin-bottom: -0.1rem;
      flex-wrap: wrap;
    }

    @media (max-width: 1200px) {
      .title-container {
        flex-direction: column;
        gap: 1rem;
      }
      
      .hero h1 {
        min-width: auto;
        text-align: center;
      }
    }

    .authors {
      font-size: 1.1rem;
      margin: -0.5rem 0;
      color: #333;
    }

    .authors strong {
      display: block;
      margin-bottom: 0.0rem;
      font-weight: 600;
    }

    .institution {
      font-size: 1.2rem;
      font-weight: 500;
      color: #333;
      margin: 0.3rem 0;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      gap: 0.8rem;
    }

    .institution-logos {
      display: flex;
      align-items: center;
      gap: 1.2rem;
    }

    .institution-logo {
      height: 50px;
      width: auto;
      object-fit: contain;
      max-width: 150px;
      display: block;
    }

    /* Ensure consistent height for both logos */
    .institution-logo:first-child {
      height: 50px;
    }
    
    .institution-logo:last-child {
      height: 50px;
    }

    .institution-text {
      text-align: center;
    }

    .paper-links {
      display: flex;
      gap: 1rem;
      justify-content: center;
      margin-top: 1.5rem;
    }

    .btn {
      padding: 8px 18px;
      border: none;
      border-radius: 15px;
      font-weight: 600;
      text-decoration: none;
      display: inline-flex;
      align-items: center;
      gap: 0.5rem;
      transition: all 0.3s ease;
      cursor: pointer;
      font-size: 1rem;
    }

    .btn i {
      font-size: 0.85rem;
    }

    /* Special styling for buttons with coming soon text */
    .btn.btn-coming-soon {
      flex-direction: column;
      text-align: center;
      gap: 0.05rem;
      padding: 6px 16px;
      line-height: 1.2;
    }

    .btn.btn-coming-soon .btn-main-content {
      display: flex;
      align-items: center;
      gap: 0.4rem;
    }

    .btn small {
      font-size: 0.6rem;
      opacity: 0.8;
      margin-top: 0.02rem;
      line-height: 1.1;
    }

    .btn-primary {
      background: #667eea;
      color: white;
      border: 2px solid #667eea;
    }

    .btn-primary:hover {
      background: #5a67d8;
      transform: translateY(-2px);
    }

    .btn-secondary {
      background: white;
      color: #667eea;
      border: 2px solid transparent;
    }

    .btn-secondary:hover {
      background: #f8fafc;
      transform: translateY(-2px);
    }

    /* Main Content */
    .main-content {
      background: #f8fafc;
      position: relative;
    }

    .container {
      max-width: 1120px;
      margin: 0 auto;
      padding: 0.3rem 1.5rem; /* Reduced top padding to show content earlier */
    }

    .section {
      background: white;
      margin-bottom: 0.5rem; /* Reduced margin to show more content */
      padding: 1rem; /* Slightly reduced padding */
      border-radius: 12px;
      box-shadow: 0 3px 15px rgba(0, 0, 0, 0.06);
      transition: transform 0.3s ease, box-shadow 0.3s ease;
    }

    .section:hover {
      transform: translateY(-2px);
      box-shadow: 0 8px 30px rgba(0, 0, 0, 0.12);
    }

    .section h2 {
      color: #1e293b;
      font-size: 1.6rem;
      font-weight: 700;
      margin: 0 0 0.8rem;
      display: flex;
      align-items: center;
      gap: 0.6rem;
    }

    .section h2 i {
      color: #667eea;
      font-size: 1.2rem;
    }

    .section h3 {
      color: #334155;
      font-size: 1.2rem;
      font-weight: 600;
      margin: 1rem 0 0.6rem;
      border-left: 3px solid #667eea;
      padding-left: 0.8rem;
    }

    .section p {
      font-size: 1rem;
      line-height: 1.6;
      color: #475569;
      margin: 0.8rem 0;
      text-align: justify;
      text-justify: inter-word;
    }

    /* Additional spacing optimizations */
    .benchmark-tables h3 {
      margin-top: 1.5rem;
    }
    
    .benchmark-tables h3:first-child {
      margin-top: 0;
    }

    /* Two-column table grid layout */
    .table-grid {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 2rem;
      margin: 1.5rem 0;
    }

    .table-grid-item {
      background: white;
      border-radius: 8px;
      box-shadow: 0 1px 6px rgba(0, 0, 0, 0.08);
      overflow: hidden;
    }

    .table-grid-item h4 {
      background: linear-gradient(135deg, #667eea, #764ba2);
      color: white;
      margin: 0;
      padding: 1rem 1rem;
      font-size: 1.1rem;
      font-weight: 600;
      text-align: center;
      border-radius: 8px 8px 0 0;
    }

    .table-grid-item .table-container {
      margin: 0;
      box-shadow: none;
      border-radius: 0 0 8px 8px;
    }

    /* Override table header styling for grid items - remove all border radius */
    .table-grid-item table {
      border-radius: 0;
    }
    
    .table-grid-item table th {
      border-radius: 0 !important;
    }

    .table-grid-item table th:first-child {
      border-radius: 0 !important;
    }

    .table-grid-item table th:last-child {
      border-radius: 0 !important;
    }

    .table-grid-item .table-container {
      border-radius: 0;
    }

    /* Responsive grid layout */
    @media (max-width: 1024px) {
      .table-grid {
        grid-template-columns: 1fr;
        gap: 1rem;
      }
    }
    
    .media-container {
      margin: 1rem 0;
    }

    /* Abstract styling */
    .abstract {
      background: linear-gradient(135deg, #f8faff 0%, #f1f5f9 100%);
      border-left: 3px solid #667eea;
      position: relative;
      border-top: 1px solid rgba(102, 126, 234, 0.1);
    }
    
    .abstract::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      height: 1px;
      background: linear-gradient(90deg, rgba(102, 126, 234, 0.3) 0%, transparent 100%);
    }

    /* Method section styling - removed to match Results section */

    /* Key innovations styling */
    .key-innovations-title {
      color: #1e293b;
      font-size: 1.2rem;
      font-weight: 600;
      margin: 1.5rem 0 1rem;
      display: flex;
      align-items: center;
      gap: 0.6rem;
    }

    .key-innovations-title i {
      color: #667eea;
      font-size: 1rem;
    }

    /* Table styling */
    .table-container {
      overflow-x: auto;
      margin: 1rem 0;
      border-radius: 8px;
      box-shadow: 0 1px 6px rgba(0, 0, 0, 0.08);
    }

    table {
      width: 100%;
      border-collapse: collapse;
      background: white;
      table-layout: fixed;
      font-size: 0.9rem;
    }

    th {
      background: linear-gradient(135deg, #667eea, #764ba2);
      color: white;
      padding: 0.6rem 0.4rem;
      text-align: left;
      font-weight: 600;
      font-size: 0.85rem;
      text-transform: uppercase;
      letter-spacing: 0.3px;
    }

    /* Set fixed width for the first column and equal width for remaining columns */
    th:first-child,
    td:first-child {
      width: 25%;
    }

    /* For 5-column tables (most tables have 5 columns) */
    table:not(.six-column) th:not(:first-child),
    table:not(.six-column) td:not(:first-child) {
      width: 18.75%; /* (100% - 25%) / 4 = 18.75% */
    }

    /* For 6-column tables (Ablation Study table) */
    table.six-column th:first-child,
    table.six-column td:first-child {
      width: 25%;
    }

    table.six-column th:not(:first-child),
    table.six-column td:not(:first-child) {
      width: 15%; /* (100% - 25%) / 5 = 15% */
    }

    td {
      padding: 0.6rem 0.4rem;
      border-bottom: 1px solid #e2e8f0;
      color: #475569;
      font-size: 0.85rem;
    }

    /* Center align all table cells except the first column */
    th:not(:first-child),
    td:not(:first-child) {
      text-align: center;
    }

    tr:hover {
      background: #f8fafc;
    }

    /* Media styling */
    .media-container {
      margin: 1rem 0;
      padding: 0;
      border-radius: 0;
      overflow: visible;
      background: transparent;
      border: none;
      box-shadow: none;
      transition: none;
    }

    .media-container:hover {
      box-shadow: none;
      transform: none;
    }

    .media-container img {
      max-width: 85%;
      width: auto;
      height: auto;
      display: block;
      margin: 0 auto;
      border: none;
      background: transparent;
      border-radius: 0;
      box-shadow: none;
      transition: none;
    }

    .media-container img:hover {
      transform: none;
      box-shadow: none;
    }

    .image-placeholder, .video-placeholder {
      aspect-ratio: 16/9;
      background: linear-gradient(135deg, #e2e8f0 0%, #cbd5e1 100%);
      display: flex;
      align-items: center;
      justify-content: center;
      color: #64748b;
      font-size: 1.2rem;
      position: relative;
    }

    .image-placeholder i, .video-placeholder i {
      font-size: 3rem;
      margin-bottom: 1rem;
    }

    .placeholder-text {
      text-align: center;
    }

    .video-container {
      position: relative;
      padding-bottom: 56.25%;
      height: 0;
      overflow: hidden;
      background: #000;
    }

    .video-container iframe, .video-container video {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
    }

    /* Video Grid Layout */
    .video-grid {
      display: grid;
      grid-template-columns: repeat(4, 1fr);
      gap: 1rem;
      margin: 1.5rem 0;
    }

    /* Special layout for Real World Experiments - left video spans 2 rows, right side 2x3 grid */
    .video-grid-special {
      display: grid;
      grid-template-columns: 1fr 1fr 1fr 1fr;
      grid-template-rows: 1fr 1fr;
      gap: 1rem;
      margin: 1.5rem 0;
      height: 540px; /* Further increased height for better caption spacing */
    }

    .video-item-large {
      grid-column: 1;
      grid-row: 1 / 3; /* Span both rows */
      background: white;
      border-radius: 8px;
      overflow: hidden;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
      transition: transform 0.3s ease, box-shadow 0.3s ease;
    }

    .video-item-large:hover {
      transform: translateY(-2px);
      box-shadow: 0 4px 15px rgba(0, 0, 0, 0.15);
    }

    .video-item-large video {
      width: 100%;
      height: calc(100% - 60px); /* Account for caption space */
      object-fit: cover;
      display: block;
    }

    .video-item {
      background: white;
      border-radius: 8px;
      overflow: hidden;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
      transition: transform 0.3s ease, box-shadow 0.3s ease;
    }

    .video-item:hover {
      transform: translateY(-2px);
      box-shadow: 0 4px 15px rgba(0, 0, 0, 0.15);
    }

    .video-item video {
      width: 100%;
      height: 200px;
      object-fit: cover;
      display: block;
    }

    /* Special styling for video items in grid-special layout */
    .video-grid-special .video-item video {
      height: calc(100% - 60px); /* Reserve more space for caption */
    }

    .video-caption {
      padding: 0.75rem 0.5rem;
      text-align: center;
      font-size: 0.85rem;
      font-weight: 500;
      color: #334155;
      background: #f8fafc;
      border-top: 1px solid #e2e8f0;
      min-height: 60px;
      display: flex;
      align-items: center;
      justify-content: center;
    }

    /* Responsive video grid */
    @media (max-width: 1200px) {
      .video-grid {
        grid-template-columns: repeat(2, 1fr);
      }
      
      .video-grid-special {
        grid-template-columns: 1fr 1fr;
        grid-template-rows: 1fr 1fr 1fr 1fr;
        height: auto;
      }
      
      .video-item-large {
        grid-column: 1;
        grid-row: 1 / 3;
      }

      .video-grid-special .video-item video {
        height: 180px; /* Adjusted height for better caption spacing on smaller screens */
      }
    }

    @media (max-width: 768px) {
      .video-grid {
        grid-template-columns: 1fr;
        gap: 0.8rem;
      }
      
      .video-grid-special {
        grid-template-columns: 1fr;
        grid-template-rows: auto;
        height: auto;
      }
      
      .video-item-large {
        grid-column: 1;
        grid-row: auto;
      }
      
      .video-item video {
        height: 180px;
      }
      
      .video-item-large video {
        height: 200px;
      }
    }


    /* Footer */
    .footer {
      background: #1e293b;
      color: white;
      padding: 2rem 2rem 1.5rem;
      text-align: center;
    }

    .footer p {
      margin: 0.5rem 0;
      opacity: 0.8;
    }

    /* Responsive */
    @media (max-width: 768px) {
      .nav-links {
        display: none;
      }
      
      .hero {
        padding: 2.5rem 1rem 1.5rem;
      }
      
      .container {
        padding: 1rem 0.8rem;
      }
      
      .section {
        padding: 1.2rem;
        margin-bottom: 1rem;
      }
      
      .section h2 {
        font-size: 1.4rem;
      }
      
      .section h3 {
        font-size: 1.1rem;
      }
      
      .paper-links {
        flex-direction: column;
        align-items: center;
        gap: 0.8rem;
      }
      
      .charts-grid {
        grid-template-columns: 1fr !important;
        gap: 1rem;
      }
      
      .chart-wrapper {
        height: 240px;
      }
      
      .table-container {
        font-size: 0.8rem;
      }

      th, td {
        padding: 0.4rem 0.2rem;
        font-size: 0.75rem;
      }
      
      /* Mobile responsive for media containers */
      .media-container {
        margin: 1.5rem 0;
        padding: 0;
      }
      
      .media-container img {
        max-width: 95%;
        border-radius: 0;
        box-shadow: none;
      }
      
      .media-container img:hover {
        transform: none;
        box-shadow: none;
      }
    }    /* Animation */
    @keyframes fadeInUp {
      from {
        opacity: 0;
        transform: translateY(30px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    /* Benchmarks container styling */
    .benchmarks-container {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 2rem;
      margin: 1.5rem 0;
    }

    .benchmark-item {
      text-align: center;
    }

    .benchmark-item img {
      max-width: none;
      border-radius: 0;
      box-shadow: none;
    }

    @media (max-width: 1024px) {
      .benchmarks-container {
        grid-template-columns: 1fr;
        gap: 1rem;
      }
    }

    /* Benchmark grid 2x2 layout */
    .benchmark-grid {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 1.2rem;
      margin: 1rem 0;
    }

    .benchmark-grid-item {
      background: #f8fafc;
      border-radius: 8px;
      border: 1px solid #e2e8f0;
      padding: 0.8rem;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.06);
      transition: transform 0.3s ease, box-shadow 0.3s ease;
    }

    .benchmark-grid-item:hover {
      transform: translateY(-2px);
      box-shadow: 0 8px 30px rgba(0, 0, 0, 0.12);
    }

    .benchmark-grid-item img {
      width: 100%;
      height: 160px;
      object-fit: cover;
      display: block;
    }

    .benchmark-grid-item .benchmark-title {
      padding: 0.8rem 0.8rem 0.4rem;
      font-size: 1rem;
      font-weight: 600;
      color: #334155;
      text-align: center;
      background: white;
      border-bottom: 1px solid #e2e8f0;
    }

    .benchmark-grid-item .benchmark-description {
      padding: 0.4rem 0.8rem 0.8rem;
      font-size: 0.85rem;
      color: #64748b;
      text-align: center;
      background: white;
      line-height: 1.3;
    }

    @media (max-width: 1200px) {
      .benchmark-grid[style*="grid-template-columns: 1fr 1fr 1fr"] {
        grid-template-columns: 1fr 1fr;
      }
      
      .benchmark-grid[style*="grid-template-columns: 1fr 1fr 1fr"] .benchmark-grid-item:last-child {
        grid-column: 1 / -1;
        max-width: 50%;
        margin: 0 auto;
      }
    }

    @media (max-width: 768px) {
      .benchmark-grid {
        grid-template-columns: 1fr;
        gap: 0.8rem;
      }
      
      .benchmark-grid[style*="grid-template-columns: 1fr 1fr 1fr"] {
        grid-template-columns: 1fr;
      }
      
      .benchmark-grid[style*="grid-template-columns: 1fr 1fr 1fr"] .benchmark-grid-item:last-child {
        grid-column: auto;
        max-width: none;
        margin: 0;
      }
      
      .benchmark-grid-item img {
        height: 140px;
      }
      
      .benchmark-grid-item {
        padding: 0.6rem;
      }
    }

    /* Enhanced Logo Styles with Beautiful Integration */
    .logo-container {
      position: relative;
      display: flex;
      align-items: center;
      justify-content: center;
      width: 180px;
      height: 180px;
    }

    .main-logo {
      height: 160px;
      width: auto;
      position: relative;
      z-index: 5;
    }

    .section {
      animation: fadeInUp 0.6s ease-out;
    }

    /* Chart Styles */
    .chart-container {
      margin: 1rem 0;
      padding: 1rem;
      background: #f8fafc;
      border-radius: 8px;
      border: 1px solid #e2e8f0;
    }

    .chart-title {
      font-size: 1rem;
      font-weight: 600;
      margin-bottom: 0.8rem;
      color: #334155;
      text-align: center;
    }

    .chart-wrapper {
      position: relative;
      height: 280px;
      margin-bottom: 0.5rem;
    }

    .chart-wrapper canvas {
      max-width: 100%;
      height: auto !important;
    }

    /* Compact grid layout for results section */
    .results-grid {
      display: grid;
      gap: 1rem;
      margin: 1rem 0;
    }

    .results-item {
      background: white;
      border-radius: 8px;
      overflow: hidden;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.06);
    }

    /* Two-column layout for large screens */
    @media (min-width: 1024px) {
      .charts-grid {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1.5rem;
        margin: 1rem 0;
      }
      
      .chart-container {
        margin: 0;
      }
      
      .table-row {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1.5rem;
        align-items: start;
      }
      
      .table-row .table-container {
        margin: 0;
      }
    }

    /* Three-column layout for extra large screens */
    @media (min-width: 1400px) {
      .charts-grid {
        grid-template-columns: 1fr 1fr 1fr;
      }
    }
  </style>
</head>
<body>
  <!-- Navigation -->
  <nav class="nav">
    <div class="nav-container">
      <a href="#" class="nav-brand">OmniEVA</a>
      <button class="mobile-menu-btn">
        <i class="fas fa-bars"></i>
      </button>
      <ul class="nav-links">
        <li><a href="#abstract">Abstract</a></li>
        <li><a href="#Method">Method</a></li>
        <li><a href="#benchmark">Benchmark</a></li>
        <li><a href="#results">Results</a></li>
      </ul>
    </div>
  </nav>

  <!-- Hero Section -->
  <section class="hero">
    <div class="hero-content">
      <div class="title-container">
        <div class="logo-container" style="position: relative;">
          <!-- Logo image -->
          <img src="assets/omnieva_logo_v2.png" alt="OmniEVA Logo" class="main-logo" />
        </div>
        <h1 style="margin: 0; text-align: left;">
          <span class="highlight-letter">E</span>mbodied <span class="highlight-letter">V</span>ersatile Pl<span class="highlight-letter">A</span>nner via Task-Adaptive <br>3D-Grounded and Embodiment-aware Reasoning
        </h1>
      </div>
      
      <div class="authors">
        Yuecheng Liu<sup>*</sup>,
        Dafeng Chi<sup>*</sup>, 
        Shiguang Wu<sup>*</sup>,
        Zhanguang Zhang<sup>*</sup>,
        Yuzheng Zhuang<sup>†</sup>, <br>
        Bowen Yang,
        He Zhu,
        Lingfeng Zhang,
        Pengwei Xie,
        David Gamaliel Arcos Bravo, <br>
        Yingxue Zhang,
        Jianye Hao,
        Xingyue Quan
        <br>
        <div class="institution">
          <div class="institution-logos">
            <img src="assets/huawei-logo.png" alt="Huawei Logo" class="institution-logo" />
            <img src="assets/noah-logo.png" alt="Noah's Ark Lab Logo" class="institution-logo" />
          </div>
          <div class="institution-text">Huawei Noah's Ark Lab</div>
        </div>
        <small><sup>*</sup>Equal contribution, <sup>†</sup>Corresponding author</small>
      </div>

      <div class="paper-links">
        <a href="https://arxiv.org/abs/2509.09332" class="btn btn-primary">
          <i class="fas fa-file-pdf"></i>
          Paper
        </a>
        <a href="#" class="btn btn-primary btn-coming-soon">
          <div class="btn-main-content">
            <i class="fas fa-code"></i>
            <span>Code</span>
          </div>
          <small>(coming soon)</small>
        </a>
        <a href="#" class="btn btn-primary btn-coming-soon">
          <div class="btn-main-content">
            <i class="fas fa-database"></i>
            <span>Benchmark</span>
          </div>
          <small>(coming soon)</small>
        </a>
      </div>
    </div>
  </section>

  <!-- Main Content -->
  <main class="main-content">
    <div class="container">
      <!-- Abstract -->
      <section class="section abstract" id="abstract">
        <h2>
          <i class="fas fa-bookmark"></i>
          Abstract
        </h2>
        <p>
          Recent progress in multimodal large language models (MLLMs) has unlocked new possibilities for embodied intelligence, enabling multimodal understanding, reasoning, interaction, and spatial decision-making. However, current MLLM-based systems face two major challenges: (1) <strong>the Geometric Adaptability Gap</strong>, where reliance on 2D inputs or rigid 3D geometry limits spatial generalization and adaptability; and (2) <strong>the Embodiment Constraint Gap</strong>, where physical limitations of real robots are often overlooked, leading to impractical task plans. To overcome these issues, we present <strong>OmniEVA</strong>, a versatile embodied planner featuring two core innovations: (1) a <strong>Task-Adaptive 3D Grounding</strong> mechanism with a gated router for context-sensitive 3D fusion, and (2) an <strong>Embodiment-Aware Reasoning</strong> framework that integrates task goals with physical constraints for executable planning. Extensive experiments show OmniEVA achieves state-of-the-art performance in general embodied reasoning and excels across diverse downstream tasks. Benchmark evaluations confirm its robust and flexible planning capabilities.
        </p>
      </section>

      <!-- Method -->
      <section class="section" id="Method">
        <h2>
          <i class="fas fa-lightbulb"></i>
          Method
        </h2>
        <div class="intro-content">
          <p style="margin-bottom: 1.5rem; text-align: justify; text-justify: inter-word;">
            To address these limitations, we introduce <strong>OmniEVA</strong> (Embodied Versatile Planner), a novel architecture that pioneers <strong>Task-Adaptive 3D Grounding</strong> and <strong>Embodiment-aware Reasoning</strong>. OmniEVA is the first framework to dynamically unify 2D and 3D inputs via task-conditioned feature selection, supporting both general embodied reasoning and grounded embodied task planning through two key innovations:
          </p>
          
          <h5 style="color: #334155; font-weight: 600; margin: 1.5rem 0 1rem 0; display: flex; align-items: center; gap: 0.5rem; font-size: 1.4rem; border: none; padding-left: 0;">
            <i class="fas fa-cube" style="color: #667eea;"></i>
            1. Task-Adaptive 3D Grounding
          </h5>
          <p style="margin-bottom: 1.5rem; text-align: justify; text-justify: inter-word;">
            We introduce a gated routing mechanism that dynamically modulates the infusion of 3D features into the visual-language backbone based on contextual task requirements. This allows for explicit, selective geometric grounding only when spatially essential, avoiding the drawbacks of static 3D fusion and enabling robust performance across both 2D and 3D reasoning tasks.
          </p>
          
          <div class="media-container">
            <img src="assets/model-architecture.png" alt="Model Architecture" width="100%" />
            <div style="text-align: center; margin-top: 0.8rem; color: #64748b; font-style: italic;">
              Figure 1: Detailed architecture of OmniEVA showing the Task-Adaptive 3D Grounding mechanism
            </div>
          </div>

          <h5 style="color: #334155; font-weight: 600; margin: 2rem 0 1rem 0; display: flex; align-items: center; gap: 0.5rem; font-size: 1.4rem; border: none; padding-left: 0;">
            <i class="fas fa-robot" style="color: #667eea;"></i>
            2. Embodiment-Aware Reasoning
          </h5>
          <p style="margin-bottom: 1.5rem; text-align: justify; text-justify: inter-word;">
            Moving beyond passive scene understanding, OmniEVA jointly incorporates task goals, environmental context, and physical constraints into its reasoning process. Through post-training with our proposed Task- and Embodiment-aware GRPO (TE-GRPO) algorithm, the model ensures that planning decisions account for object affordances, workspace boundaries, and kinematic feasibility, enabling decisions that are both logically coherent and physically executable.
          </p>

          <div class="media-container">
            <img src="assets/overall.png" alt="Overall Framework" width="100%" />
            <div style="text-align: center; margin-top: 0.8rem; color: #64748b; font-style: italic;">
              Figure 2: Overall framework of OmniEVA showing the complete pipeline and Embodiment-Aware Reasoning
            </div>
          </div>
        </div>
      </section>

      <!-- Benchmark -->
      <section class="section" id="benchmark">
        <h2>
          <i class="fas fa-tachometer-alt"></i>
          Benchmark
        </h2>
        <h3>Primitive Benchmarks</h3>
        <p>
          To evaluate the model's capacities in versatile embodied tasks with physical constraints, we introduce four benchmarks that connect primitive embodied capabilities with composite downstream tasks:
          <strong>Where2Go</strong>, <strong>Where2Fit</strong>, <strong>Where2Approach</strong>, and <strong>Where2Grasp</strong>. This VQA-style approach substantially reduces evaluation overhead compared to simulator-based online evaluation.
        </p>
        
        <div style="margin: 1.5rem 0; padding: 1rem; background: #f8fafc; border-left: 3px solid #667eea; border-radius: 4px;">
          <ul style="margin: 0; padding-left: 1.2rem; color: #475569;">
            <li style="margin-bottom: 0.8rem;">
              <strong>Where2Go:</strong> The agent selects the most informative next view from multiple images to locate a target object in partially observable environments. This aligns closely with <strong>Large Space Object Seeking</strong> tasks, requiring spatial layout inference and decision-making under uncertainty.
            </li>
            <li style="margin-bottom: 0.8rem;">
              <strong>Where2Fit:</strong> The agent identifies free space on tables by predicting 2D points while considering physical constraints including object location, size, and collision potential. This is highly relevant to <strong>Mobile Placement (Easy)</strong> tasks.
            </li>
            <li style="margin-bottom: 0.8rem;">
              <strong>Where2Approach:</strong> The agent identifies free table space not obstructed by chairs, demanding reasoning under occlusion and handling locomotion/manipulation constraints. This aligns with <strong>Mobile Placement (Hard)</strong> tasks in geometrically challenging scenarios.
            </li>
            <li style="margin-bottom: 0;">
              <strong>Where2Grasp:</strong> The agent identifies objects based on color, size, location, and category, emphasizing object-centric recognition and directly aligning with <strong>Mobile Pick-up</strong> task requirements.
            </li>
          </ul>
        </div>
                
        <div class="benchmark-grid">
          <div class="benchmark-grid-item">
            <div class="benchmark-title">Where2Go</div>
            <img src="assets/where2go.PNG" alt="Where2Go Benchmark" />
            <div class="benchmark-description">Prompt: From the provided visual input, identify the most informative image frame (with IDs staring from 1) that offers the best chance of locating the sota.</div>
          </div>
          <div class="benchmark-grid-item">
            <div class="benchmark-title">Where2Fit</div>
            <img src="assets/where2fit.PNG" alt="Where2Fit Benchmark" />
            <div class="benchmark-description">Prompt: Locate some free space for me on the table.</div>
          </div>
          <div class="benchmark-grid-item">
            <div class="benchmark-title">Where2Approach</div>
            <img src="assets/where2appraoch.PNG" alt="Where2Approach Benchmark" />
            <div class="benchmark-description">Prompt: Locate the closest empty spot on the table that isn't surrounded by chairs.</div>
          </div>
          <div class="benchmark-grid-item">
            <div class="benchmark-title">Where2Grasp</div>
            <img src="assets/where2grasp.PNG" alt="Where2Grasp Benchmark" />
            <div class="benchmark-description">Prompt: Locate the cola bottle on the table.</div>
          </div>
        </div>
        
        <div style="text-align: center; margin-top: 1.5rem; color: #64748b; font-style: italic;">
          Figure: Overview of the four embodied reasoning benchmarks: Where2Go, Where2Fit, Where2Approach, and Where2Grasp
        </div>

        <h3>Composite Tasks</h3>
        <p>
          To evaluate long-horizon planning capabilities, we introduce three composite mobile manipulation tasks that combine multiple primitive skills and require integrated reasoning across navigation, manipulation, and spatial understanding. Our evaluation set comprises over 30 representative scenarios featuring diverse background configurations, varying initial robot poses, and a range of object types, sizes, and locations.
        </p>
        
        <div style="margin: 1.5rem 0; padding: 1rem; background: #f8fafc; border-left: 3px solid #667eea; border-radius: 4px;">
          <ul style="margin: 0; padding-left: 1.2rem; color: #475569;">
            <li style="margin-bottom: 0.8rem;">
              <strong>Mobile Pickup:</strong> Involves grasping various objects across diverse scenes and tabletop configurations, directly building on the <strong>Where2Grasp</strong> primitive capability. The task requires precise pickup operations considering reachability and grasp constraints.
            </li>
            <li style="margin-bottom: 0.8rem;">
              <strong>Mobile Placement (Easy):</strong> The robot considers immediate table surface conditions (e.g., object occlusion) to determine optimal placement locations, leveraging skills from <strong>Where2Fit</strong> before placing the object. Navigate to a designated area and place an object at an accessible location with minimal obstacles.
            </li>
            <li style="margin-bottom: 0;">
              <strong>Mobile Placement (Hard):</strong> The robot must first determine optimal chassis poses while accounting for environmental constraints imposed by spatial arrangements of tabletop objects and surrounding chairs, using the same challenging settings as <strong>Where2Approach</strong>. Navigate through complex environments and place objects in challenging locations with spatial constraints.
            </li>
          </ul>
        </div>
                
        <div class="benchmark-grid" style="grid-template-columns: 1fr 1fr 1fr;">
          <div class="benchmark-grid-item">
            <div class="benchmark-title">Mobile Placement (Easy)</div>
            <img src="assets/mobile_placement_easy.png" alt="Mobile Placement Easy Task" />
            <div class="benchmark-description"></div>
          </div>
          <div class="benchmark-grid-item">
            <div class="benchmark-title">Mobile Placement (Hard)</div>
            <img src="assets/mobile_placement_hard.png" alt="Mobile Placement Hard Task" />
            <div class="benchmark-description"></div>
          </div>
          <div class="benchmark-grid-item">
            <div class="benchmark-title">Mobile Pickup</div>
            <img src="assets/mobile_pickup.png" alt="Mobile Pickup Task" />
            <div class="benchmark-description"></div>
          </div>
        </div>
        
        <p style="margin-top: 1.5rem; color: #475569; font-style: italic; text-align: center;">
          The evaluation involves navigating to target poses, followed by assessing trajectory planning for safe object placement, with success rates calculated based on task completion accuracy.
        </p>
        
        <div style="text-align: center; margin-top: 1.5rem; color: #64748b; font-style: italic;">
          Figure: Overview of the three composite mobile manipulation tasks
        </div>
      </section>

      <!-- Experimental Results -->
      <section class="section" id="results">
        <h2>
          <i class="fas fa-chart-line"></i>
          Experimental Results
        </h2>
        
        <h3>Performance Visualization Overview</h3>
        
        <!-- Charts Grid Layout -->
        <div class="charts-grid">
          <!-- Chart 1: Public 2D Embodied Reasoning Benchmarks -->
          <div class="chart-container">
            <h4 class="chart-title">2D Embodied Reasoning</h4>
            <div class="chart-wrapper">
              <canvas id="chart1"></canvas>
            </div>
          </div>

          <!-- Chart 3: 3D Reasoning Benchmarks -->
          <div class="chart-container">
            <h4 class="chart-title">3D Reasoning Benchmarks</h4>
            <div class="chart-wrapper">
              <canvas id="chart3"></canvas>
            </div>
          </div>

          <!-- Chart 2: In-house Embodied Reasoning Benchmarks -->
          <div class="chart-container">
            <h4 class="chart-title">In-house Embodied Reasoning</h4>
            <div class="chart-wrapper">
              <canvas id="chart2"></canvas>
            </div>
          </div>
        </div>
        
        <h3>2D Reasoning Benchmarks</h3>
        <p>
          OmniEVA consistently achieves <em>state-of-the-art</em> performance across all 2D reasoning benchmarks (Where2Place, VSI-bench, PACO-LVIS, RoboRefit), surpassing significantly larger models including Robobrain-2.0-32B, GPT-4o, and Gemini-2.5-Pro. On average, it delivers a performance gain of <strong>+10.45</strong> compared with previous SOTA—Robobrain-32B.
        </p>
        <div class="media-container">
          <img src="assets/2d-reasoning-benchmarks.png" alt="2D Reasoning Benchmarks Results" width="90%" style="max-width: 1000px;" />
          <div style="text-align: center; margin-top: 0.8rem; color: #64748b; font-style: italic;">
            Figure 3: Evaluation results on 2D reasoning benchmarks
          </div>
        </div>

        <h3>3D Reasoning Benchmarks & Object Navigation Benchmarks</h3>
        <p>
          Extending to <strong>3D embodied reasoning</strong>, we evaluated OmniEVA on four widely adopted benchmarks: <strong>SQA3D</strong>, <strong>ScanQA</strong>, <strong>Scan2Cap</strong>, and <strong>ScanRefer</strong>, encompassing 3D question answering, captioning, and visual grounding tasks. OmniEVA leads on three out of four benchmarks, outperforming state-of-the-art specialized 3D LLMs with notable improvements of +2.3, +0.3, and +8.5 respectively. In 3D visual grounding (ScanRefer), OmniEVA achieves 55.8 accuracy using purely text-based input/output without external detection modules, significantly exceeding the previous best of 44.4.
        </p>
        <p>
          For <strong>Object Navigation</strong> tasks on HM3D and MP3D datasets, OmniEVA outperforms the state-of-the-art navigation model <strong>UniNavid</strong> in both success rate and path efficiency, achieving a notable +5.4 improvement in SPL by predicting optimal 3D subgoal locations for target object exploration.
        </p>
        <div class="benchmarks-container">
          <div class="benchmark-item">
            <img src="assets/3d-reasoning-benchmarks.png" alt="3D Reasoning Benchmarks Results" style="height: 400px !important; width: auto !important; border: none !important; box-shadow: none !important;" />
            <div style="text-align: center; margin-top: 0.8rem; color: #64748b; font-style: italic;">
              Figure 4: Results on 3D reasoning benchmarks
            </div>
          </div>
          <div class="benchmark-item">
            <img src="assets/objnav-benchmarks.png" alt="Object Navigation Benchmarks Results" style="height: 400px !important; width: auto !important; border: none !important; box-shadow: none !important;" />
            <div style="text-align: center; margin-top: 0.8rem; color: #64748b; font-style: italic;">
              Figure 5: Results on object navigation benchmarks
            </div>
          </div>
        </div>

        <h3>Ablation Study of the Task-Adaptive 3D-Grounding Method</h3>
        <p>
          <strong>How Effective Is the Task-Adaptive Gated Router?</strong> 
          We compared our approach against two baselines: (1) <strong>Hard-coded 3D integration:</strong> The 3D features are integrated into visual tokens for all tasks, which is a common strategy employed by prior 3D LLMs. (2) <strong>Without 3D integration:</strong> With 3D features disregarded, the model can be viewed as a traditional 2D MLLM. As shown in the ablation study results, our method outperforms both baselines in three out of four tasks, yielding an average performance improvement of 1.22%. These results underscore the model's superior adaptability and its capacity to leverage 3D information when contextually appropriate.
        </p>
        <div class="media-container">
          <img src="assets/TAGR-ablation-study.png" alt="Task-Adaptive 3D-Grounding Ablation Study Results" width="60%" style="max-width: 650px;" />
          <div style="text-align: center; margin-top: 0.8rem; color: #64748b; font-style: italic;">
            Figure 6: Ablation study results showing the effectiveness of Task-Adaptive 3D-Grounding method
          </div>
        </div>

        <p>
          <strong>When Is the TAGR Module Activated?</strong>
          To illustrate the conditions under which the <strong>Task-Adaptive Gated Router</strong> (TAGR) activates, we conducted both quantitative and qualitative analysis. First, we examined the activation probabilities of prompt words across various tasks. Language signals related to geometric attributes (e.g., "shape", "square", "rectangular") and spatial verbs (e.g., "throwing", "go", "away") consistently elicited high activation scores. This pattern suggests that such linguistic cues implicitly signal the need for 3D spatial reasoning. Conversely, prompts centered on object counting or generic inquiries (e.g., "many", "nine") exhibited low activation, implying that these tasks rely predominantly on 2D visual features.
        </p>
        <div class="media-container">
          <img src="assets/TAGR-word-activation.png" alt="TAGR Word Activation Analysis" width="100%" style="max-width: 1000px;" />
          <div style="text-align: center; margin-top: 0.8rem; color: #64748b; font-style: italic;">
            Figure 7: Word activation analysis showing activation probabilities across different prompt types
          </div>
        </div>

        <p>
          We further illustrate this behavior through qualitative case studies. In the first two examples, querying the shape of a table and a desk activates the 3D gate with differing probabilities: 0.73 for the rectangular table, indicating ambiguity between "square" and "rectangular" and thus a reliance on 3D cues; and 0.52 for the round table, suggesting sufficient 2D visual information. In contrast, object counting and color identification in the two right-hand examples leave the 3D gate inactive, demonstrating the TAGR module's ability to omit 3D features when spatial reasoning is unnecessary.
        </p>
        <div class="media-container">
          <img src="assets/TAGR-case-study.png" alt="TAGR Case Study Examples" width="90%" style="max-width: 1000px;" />
          <div style="text-align: center; margin-top: 0.8rem; color: #64748b; font-style: italic;">
            Figure 8: Qualitative case studies showing different activation scenarios of the TAGR module
          </div>
        </div>

        <h3>Ablation Study of Embodiment-aware Reasoning</h3>
        <p>
          <strong>How Effective Is the TE-GRPO Training Method?</strong> 
          Our results demonstrate that the TE-GRPO training method, which jointly optimizes both task rewards and embodiment rewards, leads to significant performance improvements on both primitive skill benchmarks and downstream tasks. Specifically, Where2Approach and Where2Fit exhibit performance gains of 28.95% and 34.28%, respectively. These improvements are also directly reflected in the increased success rates of the Mobile Placement task—by 43% in the Easy variant and 50% in the Hard variant. While both task and embodiment rewards contribute individually to performance enhancement, their combination yields maximized synergistic effects.
        </p>
        <div class="media-container">
          <img src="assets/embodiment-aware-reasoning.png" alt="Ablation Study of Embodiment-aware Reasoning Results" width="90%" style="max-width: 1000px;" />
          <div style="text-align: center; margin-top: 0.8rem; color: #64748b; font-style: italic;">
            Figure 9: Ablation study results showing the effectiveness of Embodiment-aware Reasoning framework
          </div>
        </div>

        <p>
          <strong>How does OmniEVA adapt to physical and embodiment constraints?</strong> 
          To further understand the model's embodiment awareness, we analyze OmniEVA trained with TE-GRPO. The figure below illustrates its reasoning process. OmniEVA begins by performing task-level analysis, including scenario interpretation, spatial analysis, and textual localization of vacant areas. It then incorporates physical constraints—such as reachability and workspace boundaries—to refine its predictions. Through embodiment-aware reasoning processes and reward reinforcement, the final output is a placement region that satisfies all specified requirements.
        </p>
        <div class="media-container">
          <img src="assets/embodiment-aware-case-study.png" alt="Embodiment-aware Reasoning Case Study" width="90%" style="max-width: 1000px;" />
          <div style="text-align: center; margin-top: 0.8rem; color: #64748b; font-style: italic;">
            Figure 10: Case study showing how OmniEVA adapts to physical and embodiment constraints
          </div>
        </div>

        <h3>Real World Experiments: Coffee Delivery</h3>
        <div class="video-grid-special">
          <!-- Large video on the left spanning 2 rows -->
          <div class="video-item-large">
            <video width="100%" controls>
              <source src="videos/make_coffee_v2.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <div class="video-caption">Make Coffee</div>
          </div>
          
          <!-- Top row of 3 videos on the right -->
          <div class="video-item">
            <video width="100%" height="200" controls>
              <source src="videos/square_table_place.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <div class="video-caption">Place on the cluttered square table.</div>
          </div>
          <div class="video-item">
            <video width="100%" height="200" controls>
              <source src="videos/where2approach_1.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <div class="video-caption">Find a clear area (nearby) unobstructed by chairs to place. </div>
          </div>
          <div class="video-item">
            <video width="100%" height="200" controls>
              <source src="videos/where2approach_2.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <div class="video-caption">Find a clear area (across) unobstructed by chairs to place. </div>
          </div>
          
          <!-- Bottom row of 3 videos on the right -->
          <div class="video-item">
            <video width="100%" height="200" controls>
              <source src="videos/long_table_place_1.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <div class="video-caption">Move to the long table, find the nearest clear space to place.</div>
          </div>
          <div class="video-item">
            <video width="100%" height="200" controls>
              <source src="videos/workstation_place_1.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <div class="video-caption">Move to the target workstation, place at a suitable position.</div>
          </div>
          <div class="video-item">
            <video width="100%" height="200" controls>
              <source src="videos/workstation_place_2.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <div class="video-caption">Move to the specific table and place next to the documents.</div>
          </div>
        </div>

        <h3>Real World Experiments: Large Space Obj-Seeking</h3>
        <div class="video-grid">
          <div class="video-item">
            <video width="100%" height="200" controls>
              <source src="videos/find_cup_v2.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <div class="video-caption">Find a cup for me.</div>
          </div>
          <div class="video-item">
            <video width="100%" height="200" controls>
              <source src="videos/go_to_workstation_pantry_v2.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <div class="video-caption">Go to position xx and get back to the pantry.</div>
          </div>
          <div class="video-item">
            <video width="100%" height="200" controls>
              <source src="videos/turn_and_move_v2.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <div class="video-caption">Turn right and go 2.0 meters.</div>
          </div>
          <div class="video-item">
            <video width="100%" height="200" controls>
              <source src="videos/go_to_printer.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <div class="video-caption">Go to the printer behind the pantry.</div>
          </div>
        </div>

      </section>

    </div>
  </main>

  <!-- JavaScript -->
  <script src="assets/script.js"></script>
  
  <!-- Chart.js Visualization Scripts -->
  <script>
    // Wait for DOM to be loaded
    document.addEventListener('DOMContentLoaded', function() {
      // Chart.js 配置
      Chart.defaults.font.size = 12;
      Chart.defaults.font.family = 'Inter';

      // 橘黄色系颜色配置 (从浅到深)
      const colors = {
        'OmniEVA (Ours)': '#FFE4B5',      // 浅橘黄色
        'RoboBrain2.0-32B': '#FFD700',    // 金黄色
        'RoboBrain2.0-7B': '#FFA500',     // 橙色
        'Gemini2.5-Pro': '#FF8C00',       // 深橙色
        'GPT-4o': '#FF6347',              // 橙红色
        '3DRS': '#FFE4B5',                // 浅橘黄色
        'Video-3D-LLM': '#FFD700',        // 金黄色
        'LLaVA-3D': '#FFA500',            // 橙色
        'ChatScene': '#FF8C00'            // 深橙色
      };

      // Chart 1: Public 2D Embodied Reasoning Benchmarks
      const ctx1 = document.getElementById('chart1');
      if (ctx1) {
        const chart1 = new Chart(ctx1.getContext('2d'), {
          type: 'bar',
          data: {
            labels: ['Where2Place', 'VSI-bench', 'PACO-LVIS', 'RoboRefit'],
            datasets: [
              {
                label: 'GPT-4o',
                data: [20.41, 43.60, 2.09, 9.96],
                backgroundColor: '#FFE4B5',
                borderColor: '#FFE4B5',
                borderWidth: 1
              },
              {
                label: 'Gemini2.5-Pro',
                data: [28.60, 48.83, 3.14, 17.91],
                backgroundColor: '#FFD700',
                borderColor: '#FFD700',
                borderWidth: 1
              },
              {
                label: 'RoboBrain2.0-7B',
                data: [63.59, 36.10, 11.38, 62.74],
                backgroundColor: '#FFA500',
                borderColor: '#FFA500',
                borderWidth: 1
              },
              {
                label: 'RoboBrain2.0-32B',
                data: [73.59, 42.69, 16.23, 69.98],
                backgroundColor: '#FF8C00',
                borderColor: '#FF8C00',
                borderWidth: 1
              },
              {
                label: 'OmniEVA (Ours)',
                data: [74.95, 57.17, 21.01, 91.19],
                backgroundColor: '#FF6347',
                borderColor: '#FF6347',
                borderWidth: 1
              }
            ]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            scales: {
              y: {
                beginAtZero: true,
                max: 100,
                title: {
                  display: true,
                  text: 'Performance Score'
                }
              },
              x: {
                title: {
                  display: true,
                  text: 'Benchmark'
                }
              }
            },
            plugins: {
              legend: {
                position: 'top',
                labels: {
                  boxWidth: 12,
                  padding: 15
                }
              },
              tooltip: {
                mode: 'index',
                intersect: false,
              }
            }
          }
        });
      }

      // Chart 2: In-house Embodied Reasoning Benchmarks
      const ctx2 = document.getElementById('chart2');
      if (ctx2) {
        const chart2 = new Chart(ctx2.getContext('2d'), {
          type: 'bar',
          data: {
            labels: ['Where2Go', 'Where2Fit', 'Where2Approach', 'Where2Grasp'],
            datasets: [
              {
                label: 'GPT-4o',
                data: [50.72, 37.15, 0.17, 6.38],
                backgroundColor: '#FFE4B5',
                borderColor: '#FFE4B5',
                borderWidth: 1
              },
              {
                label: 'Gemini2.5-Pro',
                data: [55.07, 41.82, 3.50, 27.00],
                backgroundColor: '#FFD700',
                borderColor: '#FFD700',
                borderWidth: 1
              },
              {
                label: 'RoboBrain2.0-7B',
                data: [38.64, 32.99, 2.85, 63.24],
                backgroundColor: '#FFA500',
                borderColor: '#FFA500',
                borderWidth: 1
              },
              {
                label: 'RoboBrain2.0-32B',
                data: [41.06, 59.23, 4.35, 67.60],
                backgroundColor: '#FF8C00',
                borderColor: '#FF8C00',
                borderWidth: 1
              },
              {
                label: 'OmniEVA (Ours)',
                data: [86.96, 78.14, 7.37, 73.91],
                backgroundColor: '#FF6347',
                borderColor: '#FF6347',
                borderWidth: 1
              }
            ]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            scales: {
              y: {
                beginAtZero: true,
                max: 100,
                title: {
                  display: true,
                  text: 'Performance Score'
                }
              },
              x: {
                title: {
                  display: true,
                  text: 'Benchmark'
                }
              }
            },
            plugins: {
              legend: {
                position: 'top',
                labels: {
                  boxWidth: 12,
                  padding: 15
                }
              },
              tooltip: {
                mode: 'index',
                intersect: false,
              }
            }
          }
        });
      }

      // Chart 3: 3D Reasoning Benchmarks
      const ctx3 = document.getElementById('chart3');
      if (ctx3) {
        const chart3 = new Chart(ctx3.getContext('2d'), {
          type: 'bar',
          data: {
            labels: ['SQA3D', 'ScanQA', 'Scan2Cap', 'ScanRefer'],
            datasets: [
              {
                label: 'ChatScene',
                data: [54.6, 21.6, 77.1, 55.5],
                backgroundColor: '#FFE4B5',
                borderColor: '#FFE4B5',
                borderWidth: 1
              },
              {
                label: 'LLaVA-3D',
                data: [55.6, 27.0, 79.2, 54.1],
                backgroundColor: '#FFD700',
                borderColor: '#FFD700',
                borderWidth: 1
              },
              {
                label: 'Video-3D-LLM',
                data: [58.6, 30.1, 83.8, 58.1],
                backgroundColor: '#FFA500',
                borderColor: '#FFA500',
                borderWidth: 1
              },
              {
                label: '3DRS',
                data: [60.6, 30.3, 86.1, 62.9],
                backgroundColor: '#FF8C00',
                borderColor: '#FF8C00',
                borderWidth: 1
              },
              {
                label: 'OmniEVA (Ours)',
                data: [62.9, 30.6, 94.6, 55.8],
                backgroundColor: '#FF6347',
                borderColor: '#FF6347',
                borderWidth: 1
              }
            ]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            scales: {
              y: {
                beginAtZero: true,
                max: 100,
                title: {
                  display: true,
                  text: 'Performance Score'
                }
              },
              x: {
                title: {
                  display: true,
                  text: 'Benchmark'
                }
              }
            },
            plugins: {
              legend: {
                position: 'top',
                labels: {
                  boxWidth: 12,
                  padding: 15
                }
              },
              tooltip: {
                mode: 'index',
                intersect: false,
              }
            }
          }
        });
      }
    });
  </script>
</body>
</html>